# ğŸ¤– Sign Language Digit Recognition using Deep CNN

<div align="center">

![License](https://img.shields.io/badge/license-Academic%20Reference-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-brightgreen.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)
![Accuracy](https://img.shields.io/badge/accuracy-97.58%25-success.svg)

*A comprehensive deep learning project implementing CNN for sign language digit recognition with extensive hyperparameter optimization and academic-grade documentation.*

</div>

## ğŸ“‹ Table of Contents
- [ğŸ¯ Project Overview](#-project-overview)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ“Š Dataset Information](#-dataset-information)
- [ğŸ—ï¸ Model Architecture](#ï¸-model-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [âš™ï¸ Hyperparameter Experiments](#ï¸-hyperparameter-experiments)
- [ğŸ“ˆ Results & Performance](#-results--performance)
- [ğŸ“š Academic Documentation](#-academic-documentation)
- [ğŸ”§ Installation & Requirements](#-installation--requirements)
- [ğŸ“ Usage Instructions](#-usage-instructions)
- [ğŸ¨ Visualizations](#-visualizations)
- [âš ï¸ Important Notes](#ï¸-important-notes)

## ğŸ¯ Project Overview

This project develops a **state-of-the-art Convolutional Neural Network** for recognizing hand gestures corresponding to digits 0-9 in sign language. The implementation features:

- âœ… **Custom CNN architecture** with progressive filter scaling
- âœ… **Comprehensive hyperparameter optimization** (11 different configurations)
- âœ… **IEEE conference paper format** technical documentation
- âœ… **Extensive evaluation metrics** (Accuracy, Precision, Recall, F1, AUC-ROC)
- âœ… **Professional visualizations** and analysis plots
- âœ… **CPU-optimized training** for accessibility

## ğŸ“ Project Structure

```
SignLanguage/
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ X.npy                          # Input images (2,062 Ã— 64Ã—64 grayscale)
â”‚   â””â”€â”€ Y.npy                          # One-hot encoded labels (2,062 Ã— 10)
â”‚
â”œâ”€â”€ ğŸ“‚ images/                         # Generated visualizations
â”‚   â”œâ”€â”€ comprehensive_analysis.png     # Complete hyperparameter analysis
â”‚   â”œâ”€â”€ confusion_matrix_baseline.png  # Baseline model confusion matrix
â”‚   â”œâ”€â”€ dataset_samples.png           # Dataset sample images
â”‚   â”œâ”€â”€ sample_images.png             # Class distribution samples
â”‚   â””â”€â”€ training_histories.png        # Training/validation curves
â”‚
â”œâ”€â”€ ğŸ cpu_cnn_experiments.py         # Main implementation & experiments
â”œâ”€â”€ ğŸ” investigate_data.py            # Dataset exploration script
â”œâ”€â”€ ğŸ“„ README.md                      # This comprehensive guide
â”œâ”€â”€ âš–ï¸ LICENSE                        # Academic Reference License
â””â”€â”€ ğŸš« .gitignore                     # Git ignore configuration
```

> **Note**: LaTeX files, virtual environments, and PDFs are excluded from the repository as per `.gitignore` configuration.

## ğŸ“Š Dataset Information

<div align="center">

| **Attribute** | **Value** |
|---------------|-----------|
| ğŸ“¸ **Total Samples** | 2,062 images |
| ğŸ–¼ï¸ **Image Dimensions** | 64Ã—64 pixels (grayscale) |
| ğŸ·ï¸ **Classes** | 10 (digits 0-9) |
| âš–ï¸ **Distribution** | Well-balanced (~206 samples per class) |
| ğŸ’¾ **Format** | NumPy arrays (.npy) |
| ğŸ“ **Input Shape** | (2062, 64, 64, 1) |
| ğŸ¯ **Output Shape** | (2062, 10) - One-hot encoded |

</div>

### Dataset Characteristics:
- ğŸ” **Preprocessing**: Normalized pixel values (0-1 range)
- ğŸ² **Data Split**: 80% training, 20% validation (stratified)
- ğŸ”„ **Augmentation**: None (clean, consistent dataset)
- âœ… **Quality**: High-quality grayscale images with clear gestures

## ğŸ—ï¸ Model Architecture

### ğŸ§  CNN Architecture Design:

```
Input (64Ã—64Ã—1)
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 1    â”‚ â† 32 filters, 3Ã—3, ReLU + BatchNorm + Dropout + MaxPool
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Conv Block 2    â”‚ â† 64 filters, 3Ã—3, ReLU + BatchNorm + Dropout + MaxPool  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Conv Block 3    â”‚ â† 128 filters, 3Ã—3, ReLU + BatchNorm + Dropout + MaxPool
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Flatten         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense (256)     â”‚ â† ReLU + Dropout
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense (128)     â”‚ â† ReLU + Dropout
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output (10)     â”‚ â† Softmax
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ Key Components:
- **ğŸ¯ Activation Functions**: ReLU (hidden layers), Softmax (output)
- **ğŸ›¡ï¸ Regularization**: Dropout (0.2-0.5), L1/L2 weight decay, Batch Normalization
- **â° Early Stopping**: Patience-based training termination
- **ğŸ“‰ Optimizer**: Adam with adaptive learning rate
- **ğŸ“Š Loss Function**: Categorical Crossentropy

## ğŸš€ Quick Start

### 1ï¸âƒ£ **Clone and Setup**
```bash
git clone https://github.com/Muhammad-Hamdan-Rauf/sign-language-digit-cnn
cd sign-language-digit-cnn
```

### 2ï¸âƒ£ **Install Dependencies**
```bash
pip install tensorflow numpy matplotlib seaborn scikit-learn pandas
```

### 3ï¸âƒ£ **Run Experiments**
```bash
python cnn_experiments.py
```

### 4ï¸âƒ£ **Explore Data** (Optional)
```bash
python investigate_data.py
```

## âš™ï¸ Hyperparameter Experiments

<div align="center">

### ğŸ§ª **11 Comprehensive Experiments Conducted**

| **Parameter** | **Values Tested** | **Best Value** |
|---------------|-------------------|----------------|
| ğŸ”¢ **Batch Size** | [16, 32, 64] | **16-32** |
| ğŸ“ˆ **Learning Rate** | [0.0005, 0.001, 0.002] | **0.001** |
| ğŸ’§ **Dropout Rate** | [0.2, 0.3, 0.5] | **0.3** |
| ğŸ”’ **L1 Regularization** | [0.0, 0.001] | **0.001** |
| ğŸ”’ **L2 Regularization** | [0.001, 0.005] | **0.005** |
| â±ï¸ **Early Stopping** | [5, 15, 20] | **15-20 epochs** |

</div>

### ğŸ“‹ **Experiment Configurations:**
1. **Baseline Model** - Default parameters
2. **Batch Size Variations** - 16, 32, 64
3. **Learning Rate Tuning** - 0.0005, 0.001, 0.002  
4. **Dropout Optimization** - 0.2, 0.3, 0.5
5. **L1 Regularization** - Î» = 0.001
6. **L2 Regularization** - Î» = 0.005
7. **Early Stopping** - Patience variations

## ğŸ“ˆ Results & Performance

### ğŸ† **Top Performing Models:**

<div align="center">

| **Rank** | **Model** | **Accuracy** | **Precision** | **Recall** | **F1-Score** | **AUC-ROC** |
|----------|-----------|--------------|---------------|------------|--------------|-------------|
| ğŸ¥‡ | **Baseline** | **97.58%** | **97.61%** | **97.58%** | **97.58%** | **99.98%** |
| ğŸ¥ˆ | **Batch Size 16** | **96.85%** | **96.85%** | **96.85%** | **96.85%** | **99.93%** |
| ğŸ¥‰ | **L2 Regularization** | **96.61%** | **96.78%** | **96.59%** | **96.59%** | **99.94%** |
| 4ï¸âƒ£ | **L1 Regularization** | **96.13%** | **96.17%** | **96.12%** | **96.10%** | **99.91%** |

</div>

### ğŸ’¡ **Key Insights:**
- âœ… **Exceptional Performance**: 97.58% accuracy achieved
- âœ… **Robust Generalization**: High AUC-ROC scores (>99.9%)
- âœ… **Balanced Metrics**: Consistent precision, recall, and F1-scores
- âœ… **Optimal Configuration**: Moderate batch sizes and dropout rates work best

## ğŸ“š Academic Documentation

This project includes comprehensive academic documentation following **IEEE conference paper standards**:

### ğŸ“„ **Report Sections:**
1. **ğŸ“ Abstract** - Methodology and results summary
2. **ğŸ¯ Introduction** - Problem statement and motivation  
3. **ğŸ“– Related Work** - Literature review and background
4. **ğŸ”¬ Methodology** - Dataset, preprocessing, and architecture
5. **ğŸ“Š Experimental Results** - Comprehensive analysis and comparisons
6. **ğŸ’­ Discussion** - Insights and practical implications
7. **âš ï¸ Limitations** - Current constraints and future work
8. **ğŸ¯ Conclusion** - Key findings and contributions

> **Note**: LaTeX source files are kept private but the methodology and results are fully documented in this README.

## ğŸ”§ Installation & Requirements

### ğŸ“‹ **System Requirements:**
- ğŸ **Python**: 3.8 or higher
- ğŸ’¾ **RAM**: 8GB+ recommended  
- âš¡ **CPU**: Multi-core processor (GPU not required but recommended)
- ğŸ’¿ **Storage**: 500MB+ free space

### ğŸ“¦ **Dependencies:**
```bash
tensorflow>=2.8.0
numpy>=1.21.0
matplotlib>=3.5.0
seaborn>=0.11.0
scikit-learn>=1.0.0
pandas>=1.3.0
```

### ğŸš€ **Installation Command:**
```bash
pip install tensorflow numpy matplotlib seaborn scikit-learn pandas
```

## ğŸ“ Usage Instructions

### ğŸ¯ **Primary Script: `cnn_experiments.py`**

This script performs the complete experimental pipeline:

1. **ğŸ“Š Data Loading & Exploration**
2. **ğŸ—ï¸ Model Architecture Setup**
3. **ğŸ¯ Baseline Model Training**
4. **âš™ï¸ Hyperparameter Experiments** (with user prompt)
5. **ğŸ“ˆ Comprehensive Evaluation**
6. **ğŸ¨ Visualization Generation**

### ğŸ” **Secondary Script: `investigate_data.py`**

Dedicated dataset exploration and analysis:
- ğŸ“Š Dataset statistics and distribution
- ğŸ–¼ï¸ Sample image visualization
- ğŸ“ˆ Class balance analysis
- ğŸ’¾ Data quality assessment

### âŒ¨ï¸ **Execution Examples:**

```bash
# Run complete experimental suite
python cnn_experiments.py

# Explore dataset characteristics
python investigate_data.py

# View generated visualizations
# Check the images/ directory for PNG files
```

## ğŸ¨ Visualizations

The project generates comprehensive visualizations stored in the `images/` directory:

### ğŸ“Š **Available Plots:**

| **Visualization** | **Description** | **Key Insights** |
|-------------------|-----------------|------------------|
| ğŸ–¼ï¸ `sample_images.png` | Dataset samples by class | Visual diversity and quality |
| ğŸ“ˆ `training_histories.png` | Training/validation curves | Convergence and overfitting analysis |
| ğŸ¯ `comprehensive_analysis.png` | Complete hyperparameter analysis | Performance comparisons |
| ğŸ” `confusion_matrix_baseline.png` | Classification breakdown | Per-class performance details |
| ğŸ“Š `dataset_samples.png` | Class distribution samples | Balance and representation |

### ğŸ¨ **Visualization Features:**
- ğŸ“Š Professional matplotlib/seaborn styling
- ğŸ¨ Clear legends and annotations
- ğŸ“ High-resolution output (300 DPI)
- ğŸ¯ Comprehensive metric coverage
- ğŸ“ˆ Interactive-style layouts

## âš ï¸ Important Notes

### ğŸ“œ **License and Usage:**
- ğŸ“– **Academic Reference License**: View-only access for educational purposes
- ğŸš« **No Redistribution**: Code cannot be copied or reused
- ğŸ“ **Academic Integrity**: Using this code in assignments may constitute plagiarism
- âœ… **Citation Required**: Appropriate attribution needed for academic references

### ğŸ” **Repository Configuration:**
- ğŸš« Virtual environments (`venv/`) excluded from repository
- ğŸš« LaTeX files (`.tex`, `.cls`, `.sty`) kept private  
- ğŸš« Generated PDFs excluded from version control
- âœ… Core implementation and visualizations included

### ğŸ¯ **Educational Purpose:**
This project demonstrates best practices in:
- ğŸ§  **Deep Learning**: CNN architecture design and optimization
- ğŸ“Š **Experimental Design**: Systematic hyperparameter exploration
- ğŸ“ **Academic Writing**: IEEE format technical documentation
- ğŸ’» **Software Engineering**: Clean code organization and documentation
- ğŸ“ˆ **Data Analysis**: Comprehensive evaluation and visualization

---

<div align="center">

### ğŸ“ **Academic Excellence | ğŸ¤– Deep Learning | ğŸ“Š Data Science**

*Developed as part of Generative AI coursework - Semester 7*

**â­ If this repository helped your understanding, please star it! â­**

</div>